{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDAE - MLP/CNN Tutorial on MINST",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aldolipani/SDAE19/blob/master/SDAE_MLP_CNN_Tutorial_on_MINST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLMqUCun-ylg",
        "colab_type": "text"
      },
      "source": [
        "# MLP/CNN Tutoral on MINST Datasets\n",
        "\n",
        "This is Colaboratory. Colaboratory (or briefly Colab) is a Google research project created to help disseminate machine learning education and research. It uses the Jupyter notebook environment, which you can also set up in your machines. However, the one implemented by Google requires no setup to use and runs entirely in the cloud.\n",
        "\n",
        "In this tutorial you will learn how to train MLP and CNN architectures in CPU and GPU. Today you will use the MNIST dataset. This dataset is a large database of handwritten digits. With this tutorial you will: \n",
        "\n",
        "1. familiarize with Python and a deep learning framework called PyTorch;\n",
        "2. learn how to define deep learning architectures (in both simplified and advanced mode);\n",
        "3. train architecutres and evaluate losses, and; \n",
        "4. evaluate the trained model.\n",
        "\n",
        "The task you will solve today is: given a handwritten digit, from 0 to 9, output the digit. This task is not a task for binary classifiers because the output required by the classifier is not binary but decimal. This will require the definition of a so called multi-way classifier, a 10-way classifier in this case. You can think of a 10-way classifier as 10 indipendet binary classifiers, one for each digit, which all at the same time output a prediction about the probability of having recognized their digit or not. Then, take the most confident classifier as the correct answer.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwEv9yKwGqBS",
        "colab_type": "text"
      },
      "source": [
        "##Install PyTorch\n",
        "\n",
        "The first thing you should do is to install the libraries needed for this tutorial. In the next code cell you will install torch, which is the PyTorch library, and torchvision where the helper code to download the MINST dataset is located. These libraries will be installed using the package manager of python, called `pip`. The '!' symbol is a special symbol which tells the interpreter to run the following string as a shell command and not as python code. \n",
        "\n",
        "### Tips \n",
        "\n",
        "If you have never used colab before, you can run the code cell below or by clicking on the play button located on its left-hand side or by pressing `SHIFT`+`ENTER`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtKvmZx-WmUu",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvc26FYKGxVh",
        "colab_type": "text"
      },
      "source": [
        "## Import Dependencies\n",
        "\n",
        "Now that you have installed the libraries you need, you can import them. In python this is done by using the string `import`. Read the comments below to know more about why each library is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGU6NwlsXFSt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# here you import pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# here you import torchvision\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# this library is used to print nice progress bars\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "# with this library you will plot losses\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# this is numpy! Numpy implements operations with tensors and much more\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaPiEOtaG1fR",
        "colab_type": "text"
      },
      "source": [
        "## Download MNIST Dataset\n",
        "\n",
        "First things first. Download the MINST dataset and setup train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi1ntv7uG7dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download train and validation sets\n",
        "train_set = datasets.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True)\n",
        "validation_set  = datasets.MNIST(root = './data', train = False, transform = transforms.ToTensor())\n",
        "\n",
        "# feeling like handwritten digits do not excite you enough? Try FashionMNIST by uncommenting the two lines below\n",
        "#train_set = datasets.FashionMNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True)\n",
        "#validation_set  = datasets.FashionMNIST(root = './data', train = False, transform = transforms.ToTensor())\n",
        "\n",
        "# print the number of examples in each set\n",
        "print(\"\\nnumber of train set examples:\\n\", train_set)\n",
        "print(\"number of validation set examples:\\n\", validation_set)\n",
        "\n",
        "# use the dataloader helper to generate an iterator for the train set to return random examples 5 by 5 (train batch size).\n",
        "batch_size = 5\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# use the dataloader helper to generate an iterator for the validation set to return examples 100 by 100.\n",
        "validation_loader =  torch.utils.data.DataLoader(validation_set, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA55jVRmHN-o",
        "colab_type": "text"
      },
      "source": [
        "## Analyse MNIST Dataset\n",
        "\n",
        "Let's start by visualising the first batch of examples. Run this code cell multiple times to see more examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPDJes7rHUi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "print(\"Input values (X):\")\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(\"Labels:\")\n",
        "print(' '.join('%10s' % str(train_set.classes[labels[j].item()]) for j in range(batch_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysBI4UGjIvhz",
        "colab_type": "text"
      },
      "source": [
        "Let's now print some statistics about this batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNYUqq-TIsws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Shape of the input: \", images.shape)\n",
        "print(\"Shape of the output:\", labels.shape)\n",
        "print(\"Number of classes:\", len(train_set.classes))\n",
        "print(\"Classes:\", train_set.classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s998vCqLNL9d",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters based on the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bNfVLRUYqZA",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "input_size = 784  # img_size = (28,28) ---> 28*28=784 in total\n",
        "num_classes = 10  # number of output classes discrete range [0,9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J_x6nRcNm7W",
        "colab_type": "text"
      },
      "source": [
        "# MLP in PyTorch\n",
        "\n",
        "Congrats, now you are ready to define your first MLP model. \n",
        "In PyTorch you can do this in two ways: \n",
        "\n",
        "1. the easy but limited way, or; \n",
        "2. the hard but flexible way.\n",
        "\n",
        "Let's start with the hard way.\n",
        "\n",
        "## The Hard Way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Oxu1eENmIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_sizes, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layers = nn.ModuleList() # list of layers\n",
        "    next_hidden_size = input_size # input size of the first layer, this depends on the dataset\n",
        "    for hidden_size in hidden_sizes: # for loop to define the next hidden layers\n",
        "      self.layers.append(nn.Linear(next_hidden_size, hidden_size))\n",
        "      next_hidden_size = hidden_size\n",
        "    self.output = nn.Linear(next_hidden_size, output_size) # output layer\n",
        "\n",
        "  def forward(self, x): # define the foward pass\n",
        "    out = x\n",
        "    for layer in self.layers: \n",
        "        out = layer(out)\n",
        "        out = F.relu(out) # activation function\n",
        "    out = self.output(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjbG4rAZQR0l",
        "colab_type": "text"
      },
      "source": [
        "Now you can instantiate this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCsBCXMwbpH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MLP(input_size, [400, 400, 10], num_classes)\n",
        "\n",
        "print(\"Print the structure of the model:\")\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCbKwi9cLpcN",
        "colab_type": "text"
      },
      "source": [
        "## The Easy Way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5auK1su4Lr12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, 400), # input layer\n",
        "    nn.ReLU(), # activation function\n",
        "    nn.Linear(400, 400), # first hidden layer\n",
        "    nn.ReLU(), # activation function\n",
        "    nn.Linear(400, 400), # second hidden layer\n",
        "    nn.ReLU(), # activation function\n",
        "    nn.Linear(400, num_classes)) # output layer\n",
        "\n",
        "print(\"Print the structure of the model:\")\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bYve6RNBIc",
        "colab_type": "text"
      },
      "source": [
        "# CPU or GPU?\n",
        "\n",
        "Where will you run your model? On a CPU or a GPU? The code in the cell below checks whether you have a GPU available and if yes it will load your model in it. If not, it will keep it in the CPU. By default this notebook runs on a CPU. To activate the GPU you need to open the `Runtime` menu, then click on `Change runtime type` and select `GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YAY6xmjLliU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"The model will run on\", device)\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqXcG0s0QepH",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "Now that you have a model, you can select the optimizer. For this exercise we use Stochastic Gradient Descent (SGD). To do this we need to pass which parameters of our model need to be optimized, a learning rate and a momentum.\n",
        "\n",
        "Commented you find an example of another optimizer, Adam. If you uncomment this line and comment the previous one you will use Adam optimizer to train your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePLIwvAFj2zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
        "\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m13RyibjRVkS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Loss Function\n",
        "\n",
        "The best loss for an MLP in a multi-way classification setting is the cross entropy loss. In the next code cell you will define this loss. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKjl0XbsHCDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterium = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lX0buvQHUL2",
        "colab_type": "text"
      },
      "source": [
        "#Training\n",
        "\n",
        "We use the DataLoader class to help us generate batches. Pay attention at the parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u26yxbwSGo6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Wl_dovIBNK",
        "colab_type": "text"
      },
      "source": [
        "This is an helper function that will be used to compute the loss of the model on the validation set while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u75Xa5VckuTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss_validation(model):\n",
        "  validation_loader =  torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  \n",
        "  tot_loss = 0\n",
        "  for i, batch in enumerate(validation_loader):\n",
        "    batch = [item.to(device) for item in batch] # we move the batch to the device\n",
        "    images, labels = batch\n",
        "    xs = images.view(-1, input_size)\n",
        "    ys = labels\n",
        "    \n",
        "    pred_ys = model(xs) # generate the predictions using the model\n",
        "    loss = criterium(pred_ys, ys) # evaluate the predictions using the cross entropy loss\n",
        "    tot_loss += loss.item() # get the number and sum it to the total loss\n",
        "    \n",
        "    if (i+1) % 200 == 0:\n",
        "      break\n",
        "    \n",
        "  loss = tot_loss / 200 # normalize the loss based on the number of testing examples\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImIKZGUuG6hi",
        "colab_type": "text"
      },
      "source": [
        "Now you define how many times your model will be trained on the entire training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6LOuEEBGlQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10 # how many times you want to train on the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKTOAaNkJMOa",
        "colab_type": "text"
      },
      "source": [
        "Now you will first set the model to training mode. This means that the provided examples will be used for training. Then, the model will loop over two for loops, one over the epochs and one over the batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ios2SArQJMnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "model.train() # set the model to training mode\n",
        "\n",
        "# these variables are used to store the losses\n",
        "running_loss = 0\n",
        "training_loss = []\n",
        "validating_loss = []\n",
        "# loop over the epochs\n",
        "for epoch in range(num_epochs):\n",
        "  # loop over the batches\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    batch = [item.to(device) for item in batch] # move the batch to the device\n",
        "    images, labels = batch # extract images and labels\n",
        "    xs = images.view(-1, input_size) # flatten the image into a vector\n",
        "    ys = labels\n",
        "    \n",
        "    optimizer.zero_grad() # reset the gradients\n",
        "    pred_ys = model(xs) # generate the predictions\n",
        "    loss = criterium(pred_ys, ys) # compute the loss\n",
        "    loss.backward() # backpropagation\n",
        "    optimizer.step() # optmizes here\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "    if (i+1) % 200 == 0: # every 100 batches print statistics about the training\n",
        "      running_loss /= 200 # training loss on the last batch\n",
        "      training_loss.append(running_loss) # keep track of the training loss\n",
        "      model.eval() # set the model to eval, we can now test the model \n",
        "      validation_loss = compute_loss_validation(model) # compute the validation loss\n",
        "      validating_loss.append(validation_loss) # keep track of the validation loss\n",
        "      model.train() # set back the model to train\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Train Loss: %.4f, Validation Loss: %.4f'%(epoch+1, num_epochs, i+1, len(train_set)//batch_size, running_loss, validation_loss))\n",
        "      running_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Y4Gk7-YZas",
        "colab_type": "text"
      },
      "source": [
        "# Analyse Losses\n",
        "\n",
        "You can now plot the train and validation losses. However, in order to get a better plot we need to shift the training loss by one. This because, although the training loss and validation loss are computed over 200 batches, the training loss is used to improve the model. This means that its loss decreases in the process, while this does not happen for the validation loss where this loss is computed with the current best model. Hence, the validation loss may become lower than the training loss. To avoid this artifact, we shift forward the validation loss by one.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmalQfEobK-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_loss.pop()\n",
        "validating_loss.insert(0, float('NaN'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAlAIjY_bNtL",
        "colab_type": "text"
      },
      "source": [
        "Now you can plot the losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V31BiXPYcr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(training_loss, label=\"train\")\n",
        "plt.plot(validating_loss, label=\"validation\")\n",
        "plt.legend(loc='upper right')\n",
        "for i in range(num_epochs):\n",
        "  plt.axvline(x=(len(train_set)//batch_size)*(i+1)/200,color='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1pn6Ek2dkNG",
        "colab_type": "text"
      },
      "source": [
        "#Evaluating the Classfier\n",
        "\n",
        "You can evaluate this classifier using Accuracy. This is defined in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDKZiUVRb2-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(data_loader):\n",
        "  correct = 0.0 # here you will count the correct answers\n",
        "  total = 0.0 # here you will count all the answers\n",
        "  with torch.no_grad(): # ingnore the gradient graph\n",
        "    for batch in tqdm_notebook(data_loader):\n",
        "      batch = [item.to(device) for item in batch]\n",
        "      images, labels = batch\n",
        "      xs = images.view(-1, input_size)\n",
        "      ys = labels\n",
        "\n",
        "      pred_ys = model(xs).detach().cpu()\n",
        "      _, pred_ys = torch.max(pred_ys, 1)\n",
        "      correct += (pred_ys == ys.detach().cpu()).sum().numpy()\n",
        "      total += labels.size(0)\n",
        "      \n",
        "  return correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GADgrbfb9kH",
        "colab_type": "text"
      },
      "source": [
        "You can now compute the accuracy of the model on the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPvMW5jHB9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval() # set the model to evaluation\n",
        "\n",
        "train_accuracy = accuracy(train_loader)\n",
        "print('Train accuracy: {:.3f}'.format(train_accuracy))\n",
        "\n",
        "validation_accuracy = accuracy(validation_loader)\n",
        "print('Validation accuracy: {:.3f}'.format(validation_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W1OWsJPrf6Y",
        "colab_type": "text"
      },
      "source": [
        "#Convolutional Neural Networks\n",
        "\n",
        "You can now move to convolutional neural networks. \n",
        "\n",
        "## The Hard Way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P6qQh-QJ9nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, conv_hidden_sizes:list, linear_hidden_sizes, output_size):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv_layers = nn.ModuleList()\n",
        "    next_conv_size = 1\n",
        "    for conv_hidden_size in conv_hidden_sizes:\n",
        "      conv = nn.Conv2d(next_conv_size, conv_hidden_size, kernel_size=5)\n",
        "      self.conv_layers.append(conv)\n",
        "      next_conv_size = conv_hidden_size\n",
        "    self.linear_layers = nn.ModuleList()\n",
        "    next_linear_size = 3*3*next_conv_size\n",
        "    for linear_hidden_size in linear_hidden_sizes:\n",
        "      linear = nn.Linear(next_linear_size, linear_hidden_size)\n",
        "      self.linear_layers.append(linear)\n",
        "      next_linear_size = linear_hidden_size\n",
        "    self.output = nn.Linear(linear_hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = x\n",
        "    for i, layer in enumerate(self.conv_layers):\n",
        "      out = layer(out)\n",
        "      if i > 0:\n",
        "        out = F.max_pool2d(out, 2)\n",
        "      out = F.relu(out)    \n",
        "    out = out.view(-1,3*3*64)\n",
        "    for layer in self.linear_layers:\n",
        "      out = layer(out)\n",
        "      out = F.relu(out)\n",
        "    out = self.output(out)\n",
        "    return out\n",
        "\n",
        "model = CNN([32, 32, 64], [256], 10)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmT3Dgu-crKJ",
        "colab_type": "text"
      },
      "source": [
        "## The Easy Way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7UJQbWFcqZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size()[0], -1)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size = 5), \n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 32, kernel_size = 5), \n",
        "    nn.MaxPool2d(2),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, kernel_size = 5), \n",
        "    nn.MaxPool2d(2),\n",
        "    nn.ReLU(),\n",
        "    Flatten(),\n",
        "    nn.Linear(3*3*64, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 10))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgZJ7aqzeYCU",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrNyHKfXroZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss_validation(model):\n",
        "  validation_loader =  torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  \n",
        "  tot_loss = 0\n",
        "  for i, batch in enumerate(validation_loader):\n",
        "    batch = [item.to(device) for item in batch]\n",
        "    images, labels = batch\n",
        "    xs = images\n",
        "    ys = labels\n",
        "    \n",
        "    pred_ys = model(xs) # generate the predictions using the model\n",
        "    loss = criterium(pred_ys, ys) # evaluate the predictions using the cross entropy loss\n",
        "    tot_loss += loss.item() # get the number and sum it to the total loss\n",
        "    \n",
        "    if (i+1) % 200 == 0:\n",
        "      break\n",
        "    \n",
        "  loss = tot_loss / 200 # normalize the loss based on the number of testing examples\n",
        "  return loss\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.99) #\n",
        "\n",
        "criterium = nn.CrossEntropyLoss() # loss function\n",
        "\n",
        "batch_size = 50\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "model.train() # set the model to training mode\n",
        "\n",
        "running_loss = 0\n",
        "training_loss = []\n",
        "validating_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    images, labels = batch\n",
        "    xs = images\n",
        "    ys = labels\n",
        "    \n",
        "    optimizer.zero_grad() # reset the gradients\n",
        "    pred_ys = model(xs) # generate the predictions\n",
        "    loss = criterium(pred_ys, ys) # compute the loss\n",
        "    loss.backward() # backpropagation\n",
        "    optimizer.step() # optmizes here\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "    if (i+1) % 200 == 0: # every 100 batches print statistics about the training\n",
        "      running_loss /= 200 # training loss on the last batch\n",
        "      training_loss.append(running_loss) # keep track of the training loss\n",
        "      model.eval() # set the model to eval, we can now test the model \n",
        "      validation_loss = compute_loss_validation(model) # compute the validation loss\n",
        "      validating_loss.append(validation_loss) # keep track of the validation loss\n",
        "      model.train() # set back the model to train\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Train Loss: %.4f, Validation Loss: %.4f'%(epoch+1, num_epochs, i+1, len(train_set)//batch_size, running_loss, validation_loss))\n",
        "      running_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu9zoO7PegO5",
        "colab_type": "text"
      },
      "source": [
        "# Anaylize Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWSebBl2fI5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_loss.pop()\n",
        "validating_loss.insert(0, float('NaN'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwP-no5Lejvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(training_loss, label=\"train\")\n",
        "plt.plot(validating_loss, label=\"validation\")\n",
        "plt.legend(loc='upper right')\n",
        "for i in range(num_epochs):\n",
        "  plt.axvline(x=(len(train_set)//batch_size)*(i+1)/200,color='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBbM62cprqzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(data_loader):\n",
        "  correct = 0.0\n",
        "  total = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm_notebook(data_loader):\n",
        "      batch = [item.to(device) for item in batch]\n",
        "      images, labels = batch\n",
        "      xs = images\n",
        "      ys = labels\n",
        "\n",
        "      pred_ys = model(xs).detach().cpu()\n",
        "      _, pred_ys = torch.max(pred_ys, 1)\n",
        "      correct += (pred_ys == ys.detach().cpu()).sum().numpy()\n",
        "      total += labels.size(0)\n",
        "      \n",
        "  return correct/total\n",
        "\n",
        "model.eval()\n",
        "\n",
        "train_accuracy = accuracy(train_loader)\n",
        "print('Train accuracy: {:.3f}'.format(train_accuracy))\n",
        "\n",
        "validation_accuracy = accuracy(validation_loader)\n",
        "print('Validation accuracy: {:.3f}'.format(validation_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}